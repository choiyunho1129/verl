{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13635683",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import random\n",
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import pyarrow.parquet as pq\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    cur = start.resolve()\n",
        "    for _ in range(6):\n",
        "        if (cur / \"pyproject.toml\").exists() and (cur / \"verl\").is_dir():\n",
        "            return cur\n",
        "        if cur.parent == cur:\n",
        "            break\n",
        "        cur = cur.parent\n",
        "    return start.resolve()\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd())\n",
        "\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "CKPT_DIR = REPO_ROOT / \"checkpoints/verl_grpo_critique/qwen2.5_7b_instruct_critique_llama3b_4epoch/global_step_180/actor\"\n",
        "MERGED_DIR = CKPT_DIR / \"merged_hf\"\n",
        "TRAIN_PATH = REPO_ROOT / \"data/train_critique_3.2_4.parquet\"\n",
        "\n",
        "NUM_EXAMPLES = 4\n",
        "SEED = 42\n",
        "MAX_NEW_TOKENS = 2048\n",
        "DO_SAMPLE = True\n",
        "TEMPERATURE = 0.6\n",
        "TOP_P = 0.9\n",
        "LOCAL_ONLY = False\n",
        "\n",
        "torch.set_grad_enabled(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4b153861",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running: /data1/home/yunhochoi/miniconda3/envs/verl/bin/python -m verl.model_merger merge --backend fsdp --local_dir /data1/home/yunhochoi/verl/checkpoints/verl_grpo_critique/qwen2.5_7b_instruct_critique_llama3b_4epoch/global_step_180/actor --target_dir /data1/home/yunhochoi/verl/checkpoints/verl_grpo_critique/qwen2.5_7b_instruct_critique_llama3b_4epoch/global_step_180/actor/merged_hf\n",
            "config: ModelMergerConfig(operation='merge', backend='fsdp', target_dir='/data1/home/yunhochoi/verl/checkpoints/verl_grpo_critique/qwen2.5_7b_instruct_critique_llama3b_4epoch/global_step_180/actor/merged_hf', hf_upload_path=None, private=False, test_hf_dir=None, tie_word_embedding=False, trust_remote_code=False, is_value_model=False, local_dir='/data1/home/yunhochoi/verl/checkpoints/verl_grpo_critique/qwen2.5_7b_instruct_critique_llama3b_4epoch/global_step_180/actor', hf_model_config_path='/data1/home/yunhochoi/verl/checkpoints/verl_grpo_critique/qwen2.5_7b_instruct_critique_llama3b_4epoch/global_step_180/actor/huggingface', hf_upload=False, use_cpu_initialization=False)\n",
            "Got device mesh tensor([0, 1, 2, 3], dtype=torch.int32), mesh_dim_names ('fsdp',)\n",
            "Processing model shards with 4 (4,) in total\n",
            "Saving model to /data1/home/yunhochoi/verl/checkpoints/verl_grpo_critique/qwen2.5_7b_instruct_critique_llama3b_4epoch/global_step_180/actor/merged_hf\n",
            "Saving tokenizer to /data1/home/yunhochoi/verl/checkpoints/verl_grpo_critique/qwen2.5_7b_instruct_critique_llama3b_4epoch/global_step_180/actor/merged_hf\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def has_merged_weights(path: Path) -> bool:\n",
        "    if (path / \"model.safetensors\").exists():\n",
        "        return True\n",
        "    if (path / \"pytorch_model.bin\").exists():\n",
        "        return True\n",
        "    if (path / \"model.safetensors.index.json\").exists():\n",
        "        return True\n",
        "    if any(path.glob(\"model-*-of-*.safetensors\")):\n",
        "        return True\n",
        "    if any(path.glob(\"pytorch_model-*-of-*.bin\")):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def run_merge(cmd, cwd: Path):\n",
        "    env = dict(os.environ)\n",
        "    env[\"PYTHONPATH\"] = str(cwd) + os.pathsep + env.get(\"PYTHONPATH\", \"\")\n",
        "    result = subprocess.run(\n",
        "        cmd,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        cwd=str(cwd),\n",
        "        env=env,\n",
        "    )\n",
        "    if result.returncode != 0:\n",
        "        print(\"Merge failed.\")\n",
        "        if result.stdout:\n",
        "            print(\"stdout:\\n\" + result.stdout)\n",
        "        if result.stderr:\n",
        "            print(\"stderr:\\n\" + result.stderr)\n",
        "        raise RuntimeError(f\"Merge failed with exit code {result.returncode}\")\n",
        "    if result.stdout:\n",
        "        print(result.stdout)\n",
        "\n",
        "if not has_merged_weights(MERGED_DIR):\n",
        "    MERGED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    cmd = [\n",
        "        sys.executable, \"-m\", \"verl.model_merger\", \"merge\",\n",
        "        \"--backend\", \"fsdp\",\n",
        "        \"--local_dir\", str(CKPT_DIR),\n",
        "        \"--target_dir\", str(MERGED_DIR),\n",
        "    ]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    run_merge(cmd, REPO_ROOT)\n",
        "else:\n",
        "    print(f\"Found merged HF model at {MERGED_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "06abdfaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer you are loading from '/data1/home/yunhochoi/verl/checkpoints/verl_grpo_critique/qwen2.5_7b_instruct_critique_llama3b_4epoch/global_step_180/actor/merged_hf' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.84s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(152064, 3584, padding_idx=151643)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
              "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
              "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
              "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def pick_dtype() -> torch.dtype:\n",
        "    if torch.cuda.is_available():\n",
        "        major, _ = torch.cuda.get_device_capability()\n",
        "        return torch.bfloat16 if major >= 8 else torch.float16\n",
        "    return torch.float32\n",
        "\n",
        "dtype = pick_dtype()\n",
        "device_map = \"auto\" if torch.cuda.is_available() else None\n",
        "\n",
        "tokenizer_base = AutoTokenizer.from_pretrained(\n",
        "    BASE_MODEL, trust_remote_code=True, local_files_only=LOCAL_ONLY\n",
        ")\n",
        "\n",
        "def load_tokenizer(path: str, fallback=None):\n",
        "    try:\n",
        "        return AutoTokenizer.from_pretrained(\n",
        "            path, trust_remote_code=True, local_files_only=True, use_fast=True\n",
        "        )\n",
        "    except Exception as exc:\n",
        "        print(f\"Tokenizer load failed for {path}: {exc}\")\n",
        "        if fallback is not None:\n",
        "            print(\"Falling back to base tokenizer.\")\n",
        "            return fallback\n",
        "        raise\n",
        "\n",
        "tokenizer_ft = load_tokenizer(str(MERGED_DIR), fallback=tokenizer_base)\n",
        "\n",
        "\n",
        "if tokenizer_base.pad_token is None:\n",
        "    tokenizer_base.pad_token = tokenizer_base.eos_token\n",
        "if tokenizer_ft.pad_token is None:\n",
        "    tokenizer_ft.pad_token = tokenizer_ft.eos_token\n",
        "\n",
        "model_base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=dtype,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True,\n",
        "    local_files_only=LOCAL_ONLY,\n",
        ")\n",
        "model_ft = AutoModelForCausalLM.from_pretrained(\n",
        "    str(MERGED_DIR),\n",
        "    torch_dtype=dtype,\n",
        "    device_map=device_map,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "if device_map is None:\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_base.to(device)\n",
        "    model_ft.to(device)\n",
        "\n",
        "model_base.eval()\n",
        "model_ft.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 4 critique prompts\n"
          ]
        }
      ],
      "source": [
        "CUSTOM_PROMPTS = []  # Optional: override with your own list of message dicts.\n",
        "\n",
        "if CUSTOM_PROMPTS:\n",
        "    examples = CUSTOM_PROMPTS\n",
        "else:\n",
        "    table = pq.read_table(TRAIN_PATH, columns=[\"prompt\"])\n",
        "    prompts = table.column(\"prompt\").to_pylist()\n",
        "    random.seed(SEED)\n",
        "    indices = random.sample(range(len(prompts)), k=NUM_EXAMPLES)\n",
        "    examples = [prompts[i] for i in indices]\n",
        "\n",
        "print(f\"Loaded {len(examples)} critique prompts\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def first_device(model) -> torch.device:\n",
        "    if hasattr(model, \"hf_device_map\") and model.hf_device_map:\n",
        "        for dev in model.hf_device_map.values():\n",
        "            if dev not in (\"cpu\", \"disk\", \"meta\"):\n",
        "                return torch.device(dev)\n",
        "        first = next(iter(model.hf_device_map.values()))\n",
        "        return torch.device(first)\n",
        "    return model.device\n",
        "\n",
        "def build_input(tokenizer, messages, model):\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    return input_ids.to(first_device(model))\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, tokenizer, messages):\n",
        "    input_ids = build_input(tokenizer, messages, model)\n",
        "    gen_kwargs = {\n",
        "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
        "        \"do_sample\": DO_SAMPLE,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    }\n",
        "    if DO_SAMPLE:\n",
        "        gen_kwargs.update({\"temperature\": TEMPERATURE, \"top_p\": TOP_P})\n",
        "\n",
        "    output_ids = model.generate(input_ids, **gen_kwargs)\n",
        "    new_tokens = output_ids[0, input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "def format_block(text: str, max_chars: int = 1200) -> str:\n",
        "    if len(text) <= max_chars:\n",
        "        return text\n",
        "    return text[:max_chars] + \"\\n...\\n[truncated]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Example 1"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Prompt**\n",
              "\n",
              "User Question: Let $ABCD$ be a parallelogram. We have that $M$ is the midpoint of $AB$ and $N$ is the midpoint of $BC.$ The segments $DM$ and $DN$ intersect $AC$ at $P$ and $Q$, respectively. If $AC = 7917,$ what is $QA$? [asy]  \n",
              "pair A, B, C, D, pM, pN, O, P, Q;  \n",
              "A = (25, 0) * dir(-20);  \n",
              "B = (15, 0) * dir(60);  \n",
              "C = (25, 0) * dir(160);  \n",
              "D = (15, 0) * dir(-120);  \n",
              "pM = 0.5 * A + 0.5 * B;  \n",
              "pN = 0.5 * B + 0.5 * C;  \n",
              "O = 0.25 * A + 0.25 * B + 0.25 * C + 0.25 * D;  \n",
              "P = 0.33 * C + 0.67 * A;  \n",
              "Q = 0.67 * C + 0.33 * A;  \n",
              "draw(A--B--C--D--cycle);  \n",
              "draw(A--C);  \n",
              "draw(pM--D);  \n",
              "draw(pN--D);  \n",
              "label(\"$A$\", A, S);  \n",
              "label(\"$B$\", B, NE);  \n",
              "label(\"$C$\", C, NW);  \n",
              "label(\"$D$\", D, S);  \n",
              "label(\"$M$\", pM, NE);  \n",
              "label(\"$N$\", pN, NW);  \n",
              "label(\"$P$\", P, N);  \n",
              "label(\"$Q$\", Q, NE);  \n",
              "[/asy]  \n",
              "Correct Answer: 5278  \n",
              "  \n",
              "Model Solution Trace:  \n",
              "## Step 1:  To solve this problem, we first need to understand the given information and what is being asked. We have a parallelogram $ABCD$ with midpoints $M$ on $AB$ and $N$ on $BC$. The segments $DM$ and $DN$ intersect $AC$ at $P$ and $Q$, respectively. We are given that $AC = 7917$ and we need to find the length of $QA$.  \n",
              "## Step 2:  Since $M$ and $N$ are midpoints of $AB$ and $BC$, respectively, we know that   \n",
              "...  \n",
              "[truncated]"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Base model**\n",
              "\n",
              "The solution provided contains several logical errors and misinterpretations. Let's carefully analyze the steps and identify the issues:  \n",
              "  \n",
              "## Step 1: Understanding the Problem  \n",
              "The problem correctly states that \\( M \\) is the midpoint of \\( AB \\) and \\( N \\) is the midpoint of \\( BC \\). It also mentions that \\( DM \\) and \\( DN \\) intersect \\( AC \\) at points \\( P \\) and \\( Q \\), respectively, and provides the length of \\( AC \\) as 7917. The goal is to find the length of \\( QA \\).  \n",
              "  \n",
              "## Step 2: Analyzing the Midpoints and Diagonals  \n",
              "While it is true that \\( M \\) and \\( N \\) are midpoints, the statement that \\( AC = AD \\) is incorrect. In a parallelogram, the diagonals bisect each other but do not necessarily equal the sides unless the parallelogram is a rhombus. However, this detail is not relevant to solving the problem directly.  \n",
              "  \n",
              "## Step 3: Identifying the Midpoints on \\( AC \\)  \n",
              "The solution correctly identifies that \\( P \\) and \\( Q \\) are midpoints of \\( AC \\) due to the properties of medians in triangles formed by the diagonals of the parallelogram. This means \\( AP = PC = BQ = QC = \\frac{1}{2}AC = \\frac{1}{2}(7917) = 3958.5 \\).  \n",
              "  \n",
              "## Step 4: Calculating \\( QA \\)  \n",
              "Here lies the main iss  \n",
              "...  \n",
              "[truncated]"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Fine-tuned**\n",
              "\n",
              "The solution provided contains several logical errors and misinterpretations. Let's carefully analyze the problem and provide a correct solution.  \n",
              "  \n",
              "### Correct Solution:  \n",
              "  \n",
              "Given:  \n",
              "- \\(ABCD\\) is a parallelogram.  \n",
              "- \\(M\\) is the midpoint of \\(AB\\).  \n",
              "- \\(N\\) is the midpoint of \\(BC\\).  \n",
              "- Segments \\(DM\\) and \\(DN\\) intersect \\(AC\\) at \\(P\\) and \\(Q\\), respectively.  \n",
              "- \\(AC = 7917\\).  \n",
              "  \n",
              "We need to find the length of \\(QA\\).  \n",
              "  \n",
              "#### Step-by-Step Solution:  \n",
              "  \n",
              "1. **Identify Key Properties**:  \n",
              "   - In a parallelogram, the diagonals bisect each other. So, \\(AC\\) and \\(BD\\) bisect each other at point \\(O\\).  \n",
              "   - Since \\(M\\) and \\(N\\) are midpoints, \\(AM = MB\\) and \\(BN = NC\\).  \n",
              "  \n",
              "2. **Use Similar Triangles**:  \n",
              "   - Consider triangles \\(AMD\\) and \\(CMD\\). Since \\(M\\) is the midpoint of \\(AB\\), \\(AM = MB\\).  \n",
              "   - Similarly, consider triangles \\(BND\\) and \\(CND\\). Since \\(N\\) is the midpoint of \\(BC\\), \\(BN = NC\\).  \n",
              "  \n",
              "3. **Intersection Points**:  \n",
              "   - \\(P\\) and \\(Q\\) are points where \\(DM\\) and \\(DN\\) intersect \\(AC\\), respectively.  \n",
              "   - By properties of similar triangles and the fact that \\(M\\) and \\(N\\) are midpoints, we can use the concept of mass points or Ceva's theorem to determine the ratios.  \n",
              "  \n",
              "4. **Apply Ceva  \n",
              "...  \n",
              "[truncated]"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
            "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
            "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
            "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
          ]
        }
      ],
      "source": [
        "for idx, messages in enumerate(examples, start=1):\n",
        "    prompt_text = messages[0][\"content\"]\n",
        "    base_out = generate(model_base, tokenizer_base, messages)\n",
        "    ft_out = generate(model_ft, tokenizer_ft, messages)\n",
        "\n",
        "    display(Markdown(f\"## Example {idx}\"))\n",
        "    display(Markdown(\"**Prompt**\\n\\n\" + format_block(prompt_text).replace(\"\\n\", \"  \\n\")))\n",
        "    display(Markdown(\"**Base model**\\n\\n\" + format_block(base_out).replace(\"\\n\", \"  \\n\")))\n",
        "    display(Markdown(\"**Fine-tuned**\\n\\n\" + format_block(ft_out).replace(\"\\n\", \"  \\n\")))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "verl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
